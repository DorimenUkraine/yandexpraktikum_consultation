{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yandex Praktikum PyTorch train baseline - LB 0.699"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small guide prepared for students of [Yandex Praktikum](https://praktikum.yandex.ru/) Data Science Track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Make Folds\n",
    "\n",
    "The first thing we will do is split the train into folds for cross-validation.\n",
    "\n",
    "For example, divide by 5, for validation we will use zero fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '../input/imet-2020-fgvc7/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folds(n_folds: int) -> pd.DataFrame:\n",
    "    df = pd.read_csv(DATA_ROOT + 'train.csv')\n",
    "    cls_counts = Counter(cls for classes in df['attribute_ids'].str.split()\n",
    "                         for cls in classes)\n",
    "    fold_cls_counts = defaultdict(int)\n",
    "    folds = [-1] * len(df)\n",
    "    for item in tqdm.tqdm(df.sample(frac=1, random_state=42).itertuples(),\n",
    "                          total=len(df)):\n",
    "        cls = min(item.attribute_ids.split(), key=lambda cls: cls_counts[cls])\n",
    "        fold_counts = [(f, fold_cls_counts[f, cls]) for f in range(n_folds)]\n",
    "        min_count = min([count for _, count in fold_counts])\n",
    "        random.seed(item.Index)\n",
    "        fold = random.choice([f for f, count in fold_counts\n",
    "                              if count == min_count])\n",
    "        folds[item.Index] = fold\n",
    "        for cls in item.attribute_ids.split():\n",
    "            fold_cls_counts[fold, cls] += 1\n",
    "    df['fold'] = folds\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142119/142119 [00:03<00:00, 39337.43it/s]\n"
     ]
    }
   ],
   "source": [
    "df = make_folds(n_folds=5)\n",
    "df.to_csv('folds.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use ready-made already folded folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = pd.read_csv('../input/imet2002folds/folds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>attribute_ids</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000040d66f14ced4cdd18cd95d91800f</td>\n",
       "      <td>448 2429 782</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000ef13e37ef70412166725ec034a8a</td>\n",
       "      <td>2997 3231 2730 3294 3099 2017 784</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001eeb4a06e8daa7c6951bcd124c3c7</td>\n",
       "      <td>2436 1715 23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000226398d224de78b191e6db45fd94e</td>\n",
       "      <td>2997 3433 448 782</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00029c3b0171158d63b1bbf803a7d750</td>\n",
       "      <td>3465 3322 3170 1553 781</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                      attribute_ids  fold\n",
       "0  000040d66f14ced4cdd18cd95d91800f                       448 2429 782     2\n",
       "1  0000ef13e37ef70412166725ec034a8a  2997 3231 2730 3294 3099 2017 784     0\n",
       "2  0001eeb4a06e8daa7c6951bcd124c3c7                       2436 1715 23     3\n",
       "3  000226398d224de78b191e6db45fd94e                  2997 3433 448 782     0\n",
       "4  00029c3b0171158d63b1bbf803a7d750            3465 3322 3170 1553 781     0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your dataset to load data\n",
    "\n",
    "We implement our own Dataset class for loading data. Its purpose is to load data from the disk and issue a tensor on it with the network input, label and picture identifier.\n",
    "\n",
    "Here is a link where it’s well explained how to do this with an example: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable, List\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 3474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, root: Path, df: pd.DataFrame,\n",
    "                 image_transform: Callable, debug: bool = True):\n",
    "        super().__init__()\n",
    "        self._root = root\n",
    "        self._df = df\n",
    "        self._image_transform = image_transform\n",
    "        self._debug = debug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        item = self._df.iloc[idx]\n",
    "\n",
    "        image = load_transform_image(\n",
    "            item, self._root, self._image_transform, debug=self._debug)\n",
    "        target = torch.zeros(N_CLASSES)\n",
    "        for cls in item.attribute_ids.split():\n",
    "            target[int(cls)] = 1\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class TTADataset:\n",
    "    def __init__(self, root: Path, df: pd.DataFrame,\n",
    "                 image_transform: Callable, tta: int):\n",
    "        self._root = root\n",
    "        self._df = df\n",
    "        self._image_transform = image_transform\n",
    "        self._tta = tta\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df) * self._tta\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self._df.iloc[idx % len(self._df)]\n",
    "        #print(item)\n",
    "        image = load_transform_image(item, self._root, self._image_transform)\n",
    "        return image, item.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import (\n",
    "    ToTensor, Normalize, Compose, Resize, CenterCrop, RandomCrop,\n",
    "    RandomHorizontalFlip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    RandomCrop(288),\n",
    "    RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    #RandomCrop(288),\n",
    "    RandomCrop(256),\n",
    "    RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "tensor_transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transform_image(\n",
    "        item, root: Path, image_transform: Callable, debug: bool = False):\n",
    "    image = load_image(item, root)\n",
    "    image = image_transform(image)\n",
    "    if debug:\n",
    "        image.save('_debug.png')\n",
    "    return tensor_transform(image)\n",
    "\n",
    "\n",
    "def load_image(item, root: Path) -> Image.Image:\n",
    "    image = cv2.imread(str(root + '/' + f'{item.id}.png'))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "\n",
    "def get_ids(root: Path) -> List[str]:\n",
    "    return sorted({p.name.split('_')[0] for p in root.glob('*.png')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = pd.read_csv('../input/imet2002folds/folds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold = folds[folds['fold'] != 0]\n",
    "valid_fold = folds[folds['fold'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import warnings\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import torch\n",
    "from torch import nn, cuda\n",
    "from torch.optim import Adam\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = DATA_ROOT + 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(df: pd.DataFrame, image_transform) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            TrainDataset(train_root, df, image_transform, debug=0),\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113,694 items in train, 28,425 in valid\n"
     ]
    }
   ],
   "source": [
    "train_loader = make_loader(train_fold, train_transform)\n",
    "valid_loader = make_loader(valid_fold, test_transform)\n",
    "print(f'{len(train_loader.dataset):,} items in train, '\n",
    "      f'{len(valid_loader.dataset):,} in valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import torchvision.models as M\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, x.shape[2:])\n",
    "\n",
    "\n",
    "def create_net(net_cls, pretrained: bool):\n",
    "    if pretrained:\n",
    "        net = net_cls()\n",
    "        model_name = net_cls.__name__\n",
    "        weights_path = f'../input/{model_name}/{model_name}.pth'\n",
    "        net.load_state_dict(torch.load(weights_path))\n",
    "    else:\n",
    "        net = net_cls(pretrained=pretrained)\n",
    "    return net\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes,\n",
    "                 pretrained=False, net_cls=M.resnet50, dropout=False):\n",
    "        super().__init__()\n",
    "        self.net = create_net(net_cls, pretrained=pretrained)\n",
    "        self.net.avgpool = AvgPool()\n",
    "        if dropout:\n",
    "            self.net.fc = nn.Sequential(\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(self.net.fc.in_features, num_classes),\n",
    "            )\n",
    "        else:\n",
    "            self.net.fc = nn.Linear(self.net.fc.in_features, num_classes)\n",
    "\n",
    "    def fresh_params(self):\n",
    "        return self.net.fc.parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, num_classes,\n",
    "                 pretrained=False, net_cls=M.densenet121):\n",
    "        super().__init__()\n",
    "        self.net = create_net(net_cls, pretrained=pretrained)\n",
    "        self.avg_pool = AvgPool()\n",
    "        self.net.classifier = nn.Linear(\n",
    "            self.net.classifier.in_features, num_classes)\n",
    "\n",
    "    def fresh_params(self):\n",
    "        return self.net.classifier.parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net.features(x)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        out = self.avg_pool(out).view(out.size(0), -1)\n",
    "        out = self.net.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "resnet18 = partial(ResNet, net_cls=M.resnet18)\n",
    "resnet34 = partial(ResNet, net_cls=M.resnet34)\n",
    "resnet50 = partial(ResNet, net_cls=M.resnet50)\n",
    "resnet101 = partial(ResNet, net_cls=M.resnet101)\n",
    "resnet152 = partial(ResNet, net_cls=M.resnet152)\n",
    "\n",
    "densenet121 = partial(DenseNet, net_cls=M.densenet121)\n",
    "densenet169 = partial(DenseNet, net_cls=M.densenet169)\n",
    "densenet201 = partial(DenseNet, net_cls=M.densenet201)\n",
    "densenet161 = partial(DenseNet, net_cls=M.densenet161)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "For 0.699 score you nedd train 20+ epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(num_classes=N_CLASSES, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (net): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AvgPool()\n",
       "    (fc): Linear(in_features=2048, out_features=3474, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_params = list(model.fresh_params())\n",
    "all_params = list(model.parameters())\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    " train_kwargs = dict(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            patience=4,\n",
    "            init_optimizer=lambda params, lr: Adam(params, lr),\n",
    "            use_cuda=use_cuda,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model: nn.Module, path: Path) -> Dict:\n",
    "    state = torch.load(str(path))\n",
    "    model.load_state_dict(state['model'])\n",
    "    print('Loaded model from epoch {epoch}, step {step:,}'.format(**state))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reduce_loss(loss):\n",
    "    return loss.sum() / loss.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_prediction(probabilities, threshold: float, argsorted=None,\n",
    "                        min_labels=1, max_labels=10):\n",
    "    \"\"\" Return matrix of 0/1 predictions, same shape as probabilities.\n",
    "    \"\"\"\n",
    "    assert probabilities.shape[1] == N_CLASSES\n",
    "    if argsorted is None:\n",
    "        argsorted = probabilities.argsort(axis=1)\n",
    "    max_mask = _make_mask(argsorted, max_labels)\n",
    "    min_mask = _make_mask(argsorted, min_labels)\n",
    "    prob_mask = probabilities > threshold\n",
    "    return (max_mask & prob_mask) | min_mask\n",
    "\n",
    "\n",
    "def _make_mask(argsorted, top_n: int):\n",
    "    mask = np.zeros_like(argsorted, dtype=np.uint8)\n",
    "    col_indices = argsorted[:, -top_n:].reshape(-1)\n",
    "    row_indices = [i // top_n for i in range(len(col_indices))]\n",
    "    mask[row_indices, col_indices] = 1\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(\n",
    "        model: nn.Module, criterion, valid_loader, use_cuda,\n",
    "        ) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    all_losses, all_predictions, all_targets = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valid_loader:\n",
    "            all_targets.append(targets.numpy().copy())\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            all_losses.append(_reduce_loss(loss).item())\n",
    "            predictions = torch.sigmoid(outputs)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    def get_score(y_pred):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore', category=UndefinedMetricWarning)\n",
    "            return fbeta_score(\n",
    "                all_targets, y_pred, beta=2, average='samples')\n",
    "\n",
    "    metrics = {}\n",
    "    argsorted = all_predictions.argsort(axis=1)\n",
    "    for threshold in [0.10, 0.20]:\n",
    "        metrics[f'valid_f2_th_{threshold:.2f}'] = get_score(\n",
    "            binarize_prediction(all_predictions, threshold, argsorted))\n",
    "    metrics['valid_loss'] = np.mean(all_losses)\n",
    "    print(' | '.join(f'{k} {v:.3f}' for k, v in sorted(\n",
    "        metrics.items(), key=lambda kv: -kv[1])))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model: nn.Module, criterion, *, params,\n",
    "          train_loader, valid_loader, init_optimizer, use_cuda,\n",
    "          n_epochs=None, patience=2, max_lr_changes=2) -> bool:\n",
    "    \n",
    "    lr = 1e-4\n",
    "    batch_size = 64\n",
    "    n_epochs = 1\n",
    "    params = list(params)\n",
    "    optimizer = init_optimizer(params, lr)\n",
    "\n",
    "    model_path = 'model.pt'\n",
    "    best_model_path = 'best-model.pt'\n",
    "    uptrain = False\n",
    "    if uptrain:\n",
    "        state = load_model(model, model_path)\n",
    "        epoch = state['epoch']\n",
    "        step = state['step']\n",
    "        best_valid_loss = state['best_valid_loss']\n",
    "    else:\n",
    "        epoch = 1\n",
    "        step = 0\n",
    "        best_valid_loss = float('inf')\n",
    "    lr_changes = 0\n",
    "\n",
    "    save = lambda ep: torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'epoch': ep,\n",
    "        'step': step,\n",
    "        'best_valid_loss': best_valid_loss\n",
    "    }, str(model_path))\n",
    "\n",
    "    report_each = 100\n",
    "    valid_losses = []\n",
    "    lr_reset_epoch = epoch\n",
    "    for epoch in range(epoch, n_epochs + 1):\n",
    "        model.train()\n",
    "        tq = tqdm.tqdm(total=(len(train_loader) * batch_size))\n",
    "        tq.set_description(f'Epoch {epoch}, lr {lr}')\n",
    "        losses = []\n",
    "        tl = train_loader\n",
    "        try:\n",
    "            mean_loss = 0\n",
    "            for i, (inputs, targets) in enumerate(tl):\n",
    "                if use_cuda:\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                outputs = model(inputs)\n",
    "                loss = _reduce_loss(criterion(outputs, targets))\n",
    "                batch_size = inputs.size(0)\n",
    "                (batch_size * loss).backward()\n",
    "                if (i + 1) % 1 == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    step += 1\n",
    "                tq.update(batch_size)\n",
    "                losses.append(loss.item())\n",
    "                mean_loss = np.mean(losses[-report_each:])\n",
    "                tq.set_postfix(loss=f'{mean_loss:.3f}')\n",
    "            tq.close()\n",
    "            save(epoch + 1)\n",
    "            valid_metrics = validation(model, criterion, valid_loader, use_cuda)\n",
    "            \n",
    "            valid_loss = valid_metrics['valid_loss']\n",
    "            valid_losses.append(valid_loss)\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                shutil.copy(str(model_path), str(best_model_path))\n",
    "            elif (patience and epoch - lr_reset_epoch > patience and\n",
    "                  min(valid_losses[-patience:]) > best_valid_loss):\n",
    "                lr_changes +=1\n",
    "                if lr_changes > max_lr_changes:\n",
    "                    break\n",
    "                lr /= 5\n",
    "                print(f'lr updated to {lr}')\n",
    "                lr_reset_epoch = epoch\n",
    "                optimizer = init_optimizer(params, lr)\n",
    "        except KeyboardInterrupt:\n",
    "            tq.close()\n",
    "            print('Ctrl+C, saving snapshot')\n",
    "            save(epoch)\n",
    "            print('done.')\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, lr 0.0001: 100%|█████████▉| 113694/113728 [17:50<00:00, 106.17it/s, loss=17.049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 17.076 | valid_f2_th_0.10 0.442 | valid_f2_th_0.20 0.381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(params=all_params, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
