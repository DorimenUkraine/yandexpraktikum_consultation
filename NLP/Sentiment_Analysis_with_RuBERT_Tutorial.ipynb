{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Туториал по использованию предобученной модели RuBERT в задаче классификации твитов по тональности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0214 14:12:11.676655  3532 file_utils.py:38] PyTorch version 1.4.0 available.\n",
      "C:\\Users\\dmbot\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем 5000 позитивных и негативных твитов\n",
    "\n",
    "df_tweets = pd.read_csv('./data/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# создаем токенайзер для модели BERT, для его инициализации достаточно указать словарь, на котором обучалась предобученная модель\n",
    "# BERT использует собственную токенизацию, никакой предобработки \n",
    "\n",
    "tokenizer = ppb.BertTokenizer(vocab_file='./Rubert/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизируем текст каждого твита, для BERT не требуется никакая дополнительная предобработка, лемматизация и прочее\n",
    "\n",
    "tokenized = df_tweets['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@first_timee хоть я и школота, но поверь, у нас то же самое :D общество профилирующий предмет типа)\n",
      "[101, 168, 10934, 230, 11147, 241, 1985, 358, 322, 54198, 128, 876, 37203, 128, 352, 1159, 822, 940, 2591, 156, 238, 7580, 77982, 1929, 934, 323, 7020, 2360, 122, 102]\n",
      "['@', 'first', '_', 'time', '##e', 'хоть', 'я', 'и', 'школота', ',', 'но', 'поверь', ',', 'у', 'нас', 'то', 'же', 'самое', ':', 'd', 'общество', 'профили', '##рую', '##щи', '##и', 'предмет', 'типа', ')']\n"
     ]
    }
   ],
   "source": [
    "# Пример токенизации текста: на входе - текст, а на выходе имеем массив с номерами токенов из словаря модели BERT\n",
    "\n",
    "print(df_tweets['text'][0])\n",
    "print(tokenized[0])\n",
    "print(tokenizer.tokenize(df_tweets['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0214 14:16:12.220250  3532 modeling_utils.py:456] loading weights file ./Rubert/rubert_model.bin\n"
     ]
    }
   ],
   "source": [
    "# инициализируем предобученную модель RuBERT из файла, \n",
    "# в json-файле конфигурации описаны параметры модели\n",
    "\n",
    "config = ppb.BertConfig.from_json_file('./Rubert/bert_config.json')\n",
    "model = ppb.BertModel.from_pretrained('./Rubert/rubert_model.bin', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# из-за того, что каждый твит в датасете имеет разную длину (количество токенов)\n",
    "# мы делаем паддинг - заполнение нулями каждого массива токенов до длины максимального массива\n",
    "# чтобы на выходе получить матрицу из токенизированных текстов одной длины\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 133)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим на размерность матрицы токенизированных твитов после паддинга\n",
    "\n",
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 133)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Накладываем маску на значимые токены\n",
    "# В данном случае нам важны все слова кроме нулевых токенов, появившихся на предыдущем шаге паддинга\n",
    "\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735f37ee759a475d8dc7c8a60eba7316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-d2cda029e7db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# torch.no_grad() - для ускорения инференса модели отключим рассчет градиентов\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mlast_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# в итоге собираем все эмбеддинги твитов в features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[0;32m    804\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m         )\n\u001b[0;32m    808\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m             layer_outputs = layer_module(\n\u001b[1;32m--> 423\u001b[1;33m                 \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m             )\n\u001b[0;32m    425\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     ):\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[0mself_attention_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# add self attentions if we output attention weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[0;32m    328\u001b[0m     ):\n\u001b[0;32m    329\u001b[0m         self_outputs = self.self(\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         )\n\u001b[0;32m    332\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     ):\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0mmixed_query_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1372\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1373\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# а теперь сформируем вектора текстов с помощью модели RuBERT\n",
    "\n",
    "# это не быстрый процесс, импортируем инструмент для визуализации времени обработки в цикле\n",
    "from tqdm import notebook\n",
    "\n",
    "# для того, чтобы модель отработала в условиях ограниченных ресурсов - оперативной памяти, мы разделяем входной датасет на батчи.\n",
    "# при батче в 100 твитов потребление оперативной памяти укладывается в 1Гб\n",
    "batch_size = 100\n",
    "\n",
    "# Делаем пустой список для хранения эмбеддингов (векторов) твитов\n",
    "embeddings = []\n",
    "\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        # преобразуем батч с токенизированными твитами в тензор \n",
    "        # по сути тензор - это многомерный массив, который может быть обработан нейронной сетью\n",
    "        input_ids = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        \n",
    "        # создаем тензор и для подготовленной маски\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "        # передаем в модель BERT тензор из твитов и маску - на выходе получаем эмбеддинги - вектор текста твита\n",
    "        # torch.no_grad() - для ускорения инференса модели отключим рассчет градиентов\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        # в итоге собираем все эмбеддинги твитов в features\n",
    "        embeddings.append(last_hidden_states[0][:,0,:].numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразуем список батчей эмбеддингов в numpy-матрицу \n",
    "features = np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выводим размерность полученной матрицы эмбеддингов\n",
    "# данная модель BERT формирует вектора текстов в 768-мерном пространстве признаков\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@first_timee хоть я и школота, но поверь, у нас то же самое :D общество профилирующий предмет типа)\n",
      "[-2.71498978e-01 -5.59076667e-01 -2.80984223e-01  2.64940172e-01\n",
      " -2.54880697e-01  2.69019127e-01 -4.17226553e-01  6.90964520e-01\n",
      "  9.22523886e-02 -1.33143395e-01 -5.13912886e-02 -9.64237005e-02\n",
      " -3.92212003e-01  4.07325059e-01  4.54640687e-01 -1.16573918e+00\n",
      "  3.71884644e-01  3.66923481e-01  6.32774413e-01 -2.60575920e-01\n",
      "  6.80139720e-01 -1.35592461e-01 -1.66186810e-01 -7.37247229e-01\n",
      "  1.22717768e-01 -6.80439472e-01 -9.33969021e-01 -2.73729622e-01\n",
      "  1.24252713e+00  6.58041298e-01 -4.60716672e-02  1.58906430e-01\n",
      " -9.72143590e-01  1.04688776e+00 -7.93715894e-01 -8.88676047e-02\n",
      " -1.34215206e-01 -9.63649809e-01  7.61462674e-02 -4.12987471e-02\n",
      " -6.02056459e-02  1.20653138e-01  7.70601630e-01 -4.12695259e-01\n",
      " -4.21046257e-01  5.07617950e-01  6.47019267e-01 -2.52290487e-01\n",
      "  7.04175889e-01  5.27700543e-01  1.02272320e+00 -6.13514543e-01\n",
      "  6.23782277e-01 -7.47733116e-01  1.22249615e+00 -2.15966284e-01\n",
      " -3.93527329e-01  3.33013833e-01 -1.52818903e-01  1.77407563e+00\n",
      " -9.29762006e-01  3.40532482e-01  2.93416917e-01  1.09904900e-01\n",
      "  5.64873815e-01 -2.36191183e-01 -9.11799669e-01 -4.09839571e-01\n",
      "  1.03543781e-01 -8.71096253e-01  1.77613795e-01  6.39906228e-01\n",
      " -4.14273202e-01  3.56197543e-02  3.74366939e-01 -1.86581299e-01\n",
      " -3.56414080e-01 -2.79858887e-01 -9.47762579e-02 -2.48157665e-01\n",
      "  6.88406348e-01  4.70517933e-01 -1.12605453e+00  8.62509370e-01\n",
      " -4.74901348e-01  2.38370255e-01  5.76325879e-02 -7.94721901e-01\n",
      "  1.85519546e-01  3.96762699e-01  8.31638694e-01  1.24114835e+00\n",
      " -6.58692837e-01 -2.45170921e-01 -5.48975587e-01  1.14270568e-01\n",
      "  7.14511216e-01 -7.50448942e-01  5.46217382e-01 -1.00130963e+00\n",
      "  1.36816800e-01  8.34504128e-01 -8.47849607e-01 -5.12620926e-01\n",
      " -1.56696588e-02  1.00311488e-02  3.32541198e-01  7.83997118e-01\n",
      "  1.90478370e-01 -5.54724157e-01  1.38590765e+00  5.59745431e-01\n",
      " -4.53341842e-01 -1.08609152e+00 -9.91927743e-01  4.54861164e-01\n",
      "  4.41868871e-01 -1.72858343e-01 -6.36574745e-01 -1.37812093e-01\n",
      "  1.05761755e-02  4.32396114e-01  1.35852635e+00 -1.79598033e-01\n",
      " -1.64641634e-01 -1.95095897e-01 -1.79418802e-01 -1.31084919e-01\n",
      " -8.62392485e-02  7.94346690e-01  6.41049385e-01  5.11639297e-01\n",
      " -2.11592153e-01 -2.39499748e-01  7.84015536e-01 -5.14073730e-01\n",
      "  2.09693834e-01  5.04057586e-01  3.64012510e-01  2.32848659e-01\n",
      "  1.14765012e+00 -1.86668903e-01 -1.89887658e-01  4.04293686e-01\n",
      "  7.12244928e-01 -8.79742354e-02 -6.28404200e-01  8.04115832e-03\n",
      " -5.88764012e-01 -8.71918142e-01  4.23483133e-01  1.51008308e-01\n",
      "  5.86262405e-01 -3.02331179e-01 -6.40660346e-01  8.91805470e-01\n",
      " -8.97373497e-01  9.29291964e-01 -4.38383132e-01  1.68273389e-01\n",
      " -3.09932500e-01 -5.99059537e-02 -4.15098369e-01  1.51163805e+00\n",
      "  1.01499885e-01 -5.40785134e-01 -4.11651105e-01 -1.13788748e+00\n",
      "  5.12903482e-02  4.96708572e-01 -7.60348260e-01 -1.44639030e-01\n",
      " -7.23515749e-01 -9.71051008e-02 -6.47550941e-01 -3.52749348e-01\n",
      "  7.76032135e-02 -4.50190753e-01 -3.39764535e-01 -1.14404273e+00\n",
      "  1.43719983e+00  3.42994153e-01  2.14404032e-01 -9.23669457e-01\n",
      " -2.06470355e-01  5.90526819e-01  4.10473049e-01  5.31629980e-01\n",
      "  9.18064058e-01  1.53514266e-01  1.11242545e+00  1.24977183e+00\n",
      " -6.52925491e-01 -5.51627457e-01 -6.68785989e-01  5.10875545e-02\n",
      " -9.25033689e-01 -4.74318862e-01  6.20231032e-01  5.30533552e-01\n",
      " -2.46770710e-01  1.58025622e-01  3.02623391e-01  4.03550044e-02\n",
      " -5.29717267e-01  7.14364648e-01 -7.14692414e-01 -5.97972348e-02\n",
      " -2.22954541e-01  1.13414168e+00  7.48056650e-01  1.02034473e+00\n",
      " -4.31629121e-02 -4.11169291e-01  9.33307111e-02 -4.74789321e-01\n",
      "  4.53379661e-01 -5.76546371e-01 -2.09646195e-01 -1.68216676e-02\n",
      " -1.77453086e-02  4.44784939e-01 -1.25309646e+00  6.11041673e-02\n",
      " -2.66261160e-01 -9.74363387e-01  6.90496385e-01  7.65133083e-01\n",
      "  3.82038295e-01  2.82780826e-02 -2.91756421e-01  4.19518828e-01\n",
      "  4.42076921e-01  3.01935405e-01 -6.36677518e-02 -4.65553969e-01\n",
      " -6.41067624e-01  5.74155688e-01  5.31939566e-01 -7.22336411e-01\n",
      "  2.70028830e-01  7.09937960e-02 -3.34612548e-01 -2.13608854e-02\n",
      "  1.04736783e-01 -7.70795047e-01 -3.93398494e-01 -1.55252278e-01\n",
      "  5.68240322e-02 -6.34187520e-01  3.53675038e-02 -5.23865938e-01\n",
      " -8.15669239e-01 -2.78906941e-01  1.09172873e-01  4.36062813e-01\n",
      " -4.21823740e-01  4.55998816e-04  1.02593923e+00 -4.60852921e-01\n",
      " -2.58707732e-01  1.20941377e+00 -1.00090516e+00 -5.33809185e-01\n",
      " -1.18406244e-01  1.41245887e-01 -3.37967053e-02  7.06965685e-01\n",
      "  8.79625857e-01 -6.00350976e-01  1.10734642e+00  2.44905874e-01\n",
      "  1.95768967e-01  6.07088506e-01  1.12228476e-01 -4.22032833e-01\n",
      " -3.88681322e-01  7.95072988e-02  9.15951252e-01  9.28956807e-01\n",
      "  8.96149993e-01 -1.64649814e-01  1.41101092e-01  3.07284705e-02\n",
      "  5.03329337e-01  6.18461967e-02  1.48980796e-01  3.09058249e-01\n",
      " -7.96992004e-01 -6.16694212e-01 -7.14101672e-01 -1.32090724e+00\n",
      " -1.95232391e-01 -8.56019795e-01 -1.45076215e-01  9.34530675e-01\n",
      " -9.18202162e-01  5.81841618e-02 -5.83654046e-01 -1.70043617e-01\n",
      "  2.00264156e-01  2.24737898e-01  1.21686056e-01  7.51883507e-01\n",
      "  3.35699886e-01  2.05235228e-01 -7.39112437e-01  1.73813894e-01\n",
      "  4.61718738e-01 -2.86550403e-01  2.32475981e-01 -3.47469807e-01\n",
      " -1.67995870e+00  1.42012465e+00 -2.53341705e-01 -4.97803330e-01\n",
      "  9.99328345e-02  3.17570299e-01 -4.39481527e-01  5.50098777e-01\n",
      " -8.15715373e-01 -3.76731157e-04 -2.02220023e-01  6.04813695e-01\n",
      " -3.19478452e-01  7.85504401e-01  4.08680439e-02  2.54000753e-01\n",
      " -7.26210847e-02  9.08106863e-01 -6.85231805e-01  2.81833023e-01\n",
      "  8.57702971e-01  9.42152888e-02  2.47383907e-01  1.00200817e-01\n",
      "  5.55608988e-01  8.19156319e-02  3.85147363e-01  1.09928846e-01\n",
      " -6.06437743e-01 -1.77086735e+00  1.10791720e-01 -8.79008397e-02\n",
      " -9.48525220e-02 -4.44401026e-01 -1.75859585e-01 -6.64099932e-01\n",
      "  1.34787703e+00  2.96674699e-01 -1.14126825e+00  2.67788678e-01\n",
      "  4.59635645e-01 -2.94721067e-01  8.20852935e-01  4.52128798e-01\n",
      "  1.19633198e-01 -5.41346252e-01  5.40803492e-01  5.07258058e-01\n",
      " -4.64093611e-02  1.11318612e+00 -2.50915587e-02 -5.65338135e-01\n",
      "  4.22715545e-02  1.45701480e+00 -6.65186524e-01 -1.23241663e+00\n",
      " -7.72946894e-01  1.19347644e+00 -3.52853417e-01  1.02235138e+00\n",
      "  1.04452565e-01 -1.13632035e+00 -1.00411296e-01  3.09153885e-01\n",
      " -1.19909644e+00 -7.12567449e-01  2.25629508e-01  6.60576284e-01\n",
      " -6.48920059e-01 -1.85253844e-01  1.88840270e-01  1.38601422e-01\n",
      " -1.83406308e-01 -9.77526307e-01  1.62209004e-01 -5.37591055e-02\n",
      "  6.05105639e-01 -7.84477293e-02 -1.72091082e-01  9.79448676e-01\n",
      " -1.46687114e+00 -6.82409108e-01 -5.00868201e-01 -6.31203890e-01\n",
      " -3.87962133e-01 -7.23991394e-02  8.89421761e-01 -6.78886950e-01\n",
      " -4.77981001e-01 -9.70053732e-01 -1.53187537e+00  4.57052797e-01\n",
      " -4.28984277e-02  6.75075948e-02  1.03862548e+00  1.17391932e+00\n",
      " -3.98535579e-02 -8.12240064e-01 -3.61282617e-01 -1.77483201e-01\n",
      " -4.69514638e-01 -1.03359997e+00  2.04459473e-01  1.95814595e-01\n",
      " -1.92843974e-01 -5.08322835e-01  1.74577758e-01  1.44246504e-01\n",
      "  2.61984378e-01 -5.95850587e-01 -1.25650597e+00 -3.85808349e-01\n",
      " -9.55288351e-01  9.13762808e-01  1.74118504e-02  4.90563810e-01\n",
      "  4.08330649e-01  5.07386565e-01 -4.40942407e-01  1.21535599e+00\n",
      "  4.51182812e-01 -6.25902355e-01 -3.44074965e-02 -4.11250353e-01\n",
      " -4.73445207e-01 -5.48813283e-01 -1.08991489e-01  1.16883612e+00\n",
      " -5.01085401e-01  1.55893624e-01  3.56252015e-01  4.87526387e-01\n",
      "  2.36124903e-01  2.12287068e+00  7.66756162e-02 -3.83475810e-01\n",
      " -3.26947533e-02 -1.31496966e-01 -5.03314376e-01  6.33202076e-01\n",
      "  9.13142413e-02 -8.98973107e-01 -6.13367558e-01  2.32345492e-01\n",
      " -1.36399448e-01  4.33329582e-01  6.49265349e-01 -7.19626248e-03\n",
      " -1.03994215e+00 -7.51181781e-01  4.59314436e-01  1.85594484e-01\n",
      " -1.51630908e-01 -5.76623440e-01  7.73216009e-01 -4.55250353e-01\n",
      " -4.81941581e-01  2.63017923e-01 -1.99184030e-01  7.37606585e-01\n",
      "  6.84862912e-01  5.25707781e-01 -8.73497605e-01  1.10282615e-01\n",
      " -7.76356101e-01 -3.51225249e-02  2.25018084e-01  2.47374192e-01\n",
      "  9.74456742e-02 -8.59154165e-01 -9.62346792e-03  5.32843411e-01\n",
      "  1.04329717e+00  5.96584976e-01 -1.40456498e-01  3.31049003e-02\n",
      "  1.70525551e-01 -4.63049293e-01  3.12481284e-01  8.16795453e-02\n",
      " -3.72283190e-01  3.12442571e-01 -1.15745568e+00  7.30822206e-01\n",
      "  4.84047830e-01 -4.68350261e-01 -6.38024807e-01 -2.24874765e-01\n",
      "  6.12849556e-02 -9.42224026e-01  3.09419651e-02  2.55549788e-01\n",
      "  9.21467692e-02  7.21391678e-01 -3.78406513e-03 -1.39080554e-01\n",
      " -3.21232677e-01 -5.41013062e-01  4.53949273e-02 -1.11638951e+00\n",
      " -6.05449751e-02  1.15959752e+00  6.82014167e-01  1.02379143e+00\n",
      " -3.97389948e-01 -2.69983299e-02 -1.90185070e-01  5.13756275e-03\n",
      "  5.08405387e-01 -1.94337100e-01 -1.09659031e-01 -5.58477283e-01\n",
      " -6.81500673e-01  3.91905010e-01  5.80676943e-02 -1.35440123e+00\n",
      " -3.71883690e-01 -6.93937242e-01 -6.76167428e-01 -2.18750715e-01\n",
      "  2.76621208e-02 -4.85692888e-01  4.74158674e-01 -5.47451615e-01\n",
      " -3.06571305e-01  4.13223505e-01 -6.29129350e-01 -5.66801548e-01\n",
      "  9.73234117e-01 -1.46615076e+00 -2.96071284e-02  3.86826247e-01\n",
      " -3.14519405e-01  5.62451482e-01 -3.20921242e-01 -3.29998076e-01\n",
      " -2.83838123e-01 -9.43375468e-01 -1.00864387e+00 -6.79413974e-02\n",
      "  7.16310143e-01  5.29055297e-03  3.16653758e-01  2.76766587e-02\n",
      "  5.22861183e-01 -3.58032659e-02  1.77570850e-01  7.49808788e-01\n",
      " -4.90289003e-01  1.97376877e-01 -3.92254323e-01 -4.67574388e-01\n",
      "  1.10330760e+00 -6.25127017e-01  5.48201054e-03 -9.48150530e-02\n",
      " -2.82813519e-01 -9.11905885e-01  7.92013466e-01 -3.56122524e-01\n",
      " -1.09136641e+00  3.03905427e-01 -1.25962287e-01 -2.05237851e-01\n",
      " -4.71175313e-01  3.47324729e-01  9.73873496e-01  5.06105483e-01\n",
      "  6.22142494e-01  8.11963081e-02 -3.30969900e-01  4.58909690e-01\n",
      "  3.33796144e-02  5.73847592e-01  3.69322836e-01 -4.76696417e-02\n",
      " -3.24210972e-02  7.15364695e-01 -6.39423504e-02 -7.78084755e-01\n",
      " -2.47648731e-02 -7.61901587e-02 -1.75464481e-01 -7.93303996e-02\n",
      "  4.78861332e-01  1.18502490e-01  4.46778573e-02 -4.00966227e-01\n",
      "  4.35487032e-02 -6.82443976e-01 -2.69732475e-02  3.33637267e-01\n",
      " -9.40758586e-01  6.36175424e-02  5.68700969e-01 -4.53158319e-01\n",
      "  1.82829037e-01  2.29965016e-01  1.07814454e-01  3.92019063e-01\n",
      " -5.91052413e-01 -6.97167292e-02  2.07275495e-01 -3.25986505e-01\n",
      "  1.79613546e-01 -3.51192117e-01  1.61504447e+00 -4.59998220e-01\n",
      " -2.08295599e-01  2.42858604e-01  6.27657413e-01 -1.44877708e+00\n",
      "  3.34184229e-01 -4.64658916e-01 -1.48795500e-01  1.17599063e-01\n",
      " -1.68807000e-01 -6.55207396e-01 -6.34244800e-01 -3.90434027e-01\n",
      "  1.27820477e-01  6.20665610e-01 -1.25872791e-02 -6.55780971e-01\n",
      " -4.44888532e-01  2.69233167e-01  1.15298569e-01 -1.16120607e-01\n",
      "  4.19872433e-01 -3.38532895e-01 -1.14039671e+00 -5.09863138e-01\n",
      "  1.23874140e+00  8.46749246e-01 -2.08196715e-02 -3.80423009e-01\n",
      " -8.64425659e-01  2.53637999e-01  1.14593089e+00 -1.57876503e+00\n",
      "  1.81635246e-02 -1.45371521e+00  4.54751611e-01 -4.16149944e-01\n",
      "  2.50237674e-01 -5.11155188e-01  2.37429127e-01  1.32983357e-01\n",
      "  7.40445137e-01  1.67304963e-01  8.30054879e-01 -1.29702735e+00\n",
      "  3.27654183e-01  4.58379537e-01  5.07752597e-03  2.63767928e-01\n",
      "  4.47148494e-02  7.34569550e-01 -5.16957760e-01 -8.27137679e-02\n",
      "  3.22996259e-01  7.14212894e-01 -1.53920241e-03 -2.12464333e-01\n",
      " -6.23901129e-01 -2.24797875e-01 -1.09083605e+00  1.30958557e-01\n",
      " -1.21452248e+00  5.71980357e-01 -2.28707775e-01  4.18382108e-01\n",
      "  6.97617419e-03  3.80664766e-02  7.39565566e-02 -3.92308354e-01\n",
      " -5.31269684e-02 -1.35647282e-01  2.04394698e+00  5.27297437e-01\n",
      "  4.74594012e-02 -6.82487190e-01  1.06899321e+00  1.20538417e-02\n",
      " -2.77976215e-01 -2.30473369e-01  3.36516082e-01  2.61184216e-01\n",
      "  2.24172965e-01 -1.41201258e+00 -3.63034129e-01 -1.63691118e-01\n",
      " -7.11065531e-01  4.35633510e-02  4.45618689e-01 -5.75559735e-01\n",
      "  1.46785945e-01 -6.81784809e-01  5.45984209e-01  1.08312473e-01\n",
      "  2.70908505e-01  8.50939512e-01  4.04859066e-01 -5.14132679e-02\n",
      "  4.81455684e-01  2.84429848e-01  3.78036410e-01  7.76354790e-01\n",
      "  1.22733720e-01  6.30555451e-02 -1.43264249e-01  1.41689062e-01\n",
      " -4.37302262e-01  1.17015779e-01 -7.26759315e-01  2.49262989e-01\n",
      " -8.24679017e-01 -9.79650095e-02  5.34962595e-01 -1.52787089e+00\n",
      " -5.17781556e-01  1.72815025e-01  7.74059892e-01 -6.35114133e-01\n",
      " -1.00604343e+00  3.55191767e-01  6.64502010e-02 -8.26239347e-01\n",
      " -5.56026697e-01  2.63133764e-01  4.59055096e-01 -9.36226308e-01\n",
      "  1.36518049e+00  4.34729069e-01  3.29290271e-01  9.13900018e-01\n",
      " -9.38459158e-01 -1.10599458e+00 -5.26502728e-03 -6.89643845e-02\n",
      "  4.28468883e-01 -8.60813797e-01 -7.16171205e-01  2.68104702e-01\n",
      "  1.47277951e+00  1.25719178e+00 -5.30549586e-01 -7.48531520e-03\n",
      " -1.03129923e+00 -5.94977498e-01  6.35382414e-01 -7.71854043e-01\n",
      " -8.08223724e-01 -9.20303404e-01  1.20130682e+00  1.50259435e-02]\n"
     ]
    }
   ],
   "source": [
    "# Выводим пример эмбеддинга для твита - осторожно много цифр!\n",
    "\n",
    "print(df_tweets['text'][0])\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки для обучения классификатора на logreg и оценки качества\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохраним целевую переменную: метку тональности позитив/негатив\n",
    "\n",
    "labels = df_tweets['positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Разделяем матрицу признаков и целевую переменную на обучающий и тестовый набор\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучаем классификатор на основе логистической регрессии\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: @sergeylazarev С премьерой клипа,Серёж! Свои впечатления я уже написала.Клип сильнейший! И спасибо за эфир.Подняли настроение,как всегда.))\n",
      "Predict label:  1\n",
      "True label:  1\n"
     ]
    }
   ],
   "source": [
    "# делаем пробное предсказание\n",
    "tweet_index = 555\n",
    "\n",
    "print('Text: ' + df_tweets['text'][tweet_index])\n",
    "print('Predict label: ', lr_clf.predict(features[tweet_index:tweet_index+1][:])[0])\n",
    "print('True label: ', df_tweets['positive'][tweet_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.964"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# оцениваем accuracy на тестовой выборке\n",
    "\n",
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
